{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook computes the zero-shot cross-entropy complexity measures for every model.\n",
        "\n",
        "This notebook is inspired by: https://github.com/unslothai/unsloth"
      ],
      "metadata": {
        "id": "MfLKT__CS-w3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Packages"
      ],
      "metadata": {
        "id": "8Eb5dInAgOm5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAM5NctAFwHK"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "import json\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "U4toBdXhF2yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "mxHv_a2NgSuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data"
      ],
      "metadata": {
        "id": "o8WSE1FVG1-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load and preprocess the XML file into a pandas DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load and parse XML file\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Extract data into a list of dictionaries\n",
        "    data = []\n",
        "\n",
        "    # Iterate through the XML structure and extract information from the reviews\n",
        "    for review in root.findall('Review'):\n",
        "      review_id = review.get('rid')\n",
        "\n",
        "      for sentence in review.findall('.//sentence'):\n",
        "        text = sentence.find('text').text\n",
        "\n",
        "        # Show nothing rather than 'NULL' in the prompt\n",
        "        for opinion in sentence.findall('.//Opinion'):\n",
        "          aspect = opinion.get('target')\n",
        "          if aspect == 'NULL':\n",
        "            aspect = ''\n",
        "\n",
        "          # Adjust the categories to be more informative for the prompt\n",
        "          category = opinion.get('category').lower().replace('#', ' ').replace('_',' and ')\n",
        "          if category == 'food general':\n",
        "              category = 'food style and options'\n",
        "          elif category == 'service general':\n",
        "              category = 'service'\n",
        "          elif category == 'restaurant general' or category == 'restaurant miscellaneous':\n",
        "              category = 'restaurant'\n",
        "          elif category == 'ambience general':\n",
        "              category = 'ambience'\n",
        "          elif category == 'location general':\n",
        "              category = 'location'\n",
        "\n",
        "          # Represent the aspect as 'term (category entity)'\n",
        "          aspect_term_category = aspect + ' (' + category + ')'\n",
        "\n",
        "          data.append({\n",
        "              \"sentence\": text,\n",
        "              \"aspect\": aspect,\n",
        "              \"category\": category,\n",
        "              \"aspect_term_category\": aspect_term_category,\n",
        "              \"demonstration\": demonstration,\n",
        "              \"sentiment\": opinion.get('polarity')\n",
        "          })\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    y = df['sentiment'].values\n",
        "    label_to_idx = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "    df['sentiment_label'] = df['sentiment'].map(label_to_idx)\n",
        "\n",
        "    # Return both the features DataFrame and the sentiment labels\n",
        "    return df, y"
      ],
      "metadata": {
        "id": "-AYZa4NJF5VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute zero-shot cross entropy"
      ],
      "metadata": {
        "id": "dBp9Jk5yGzxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cross_entropy(dataset):\n",
        "    \"\"\"\n",
        "    Compute the cross-entropy loss for zero-shot classification to obtain the complexity scores\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a list to store the complexity scores\n",
        "    cross_entropy = []\n",
        "\n",
        "    # Obtain the token labels of the classes\n",
        "    classes = [\"negative\", \"neutral\", \"positive\"]\n",
        "    class_token_ids = [tokenizer.encode(c, add_special_tokens=False)[0] for c in classes]\n",
        "    labels = torch.tensor(dataset['sentiment_label'], dtype=torch.long).to('cuda')\n",
        "\n",
        "    # Iterate over all training instances\n",
        "    for idx, row in enumerate(tqdm(dataset)):\n",
        "        prompt = row['prompt']\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "        # Perform inference without computing gradients to save memory\n",
        "        with torch.no_grad():\n",
        "            # This returns the outputs for all input tokens and the next one\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        # The predicted class should be at the last token (= logit.size(1) -1)\n",
        "        logits = outputs.logits\n",
        "        prediction_position = logits.size(1) - 1\n",
        "\n",
        "        # Retrieve the logits for the classes at the specified position (end of prompt)\n",
        "        logits_for_prediction = logits[:, prediction_position, :]\n",
        "        class_logits = logits_for_prediction[:, class_token_ids]\n",
        "\n",
        "        # Obtain the predicted probabilities by using the softmax function\n",
        "        class_probs = torch.softmax(class_logits, dim=-1)\n",
        "        probs = class_probs.cpu().detach().float().numpy()\n",
        "\n",
        "        # Compute cross-entropy loss\n",
        "        label_idx = labels[idx].item()\n",
        "        true_class_prob = probs[0, label_idx]\n",
        "        loss = -np.log(true_class_prob)\n",
        "        cross_entropy.append(loss)\n",
        "\n",
        "        # To save memory\n",
        "        del inputs, outputs, logits, logits_for_prediction, class_logits, class_probs\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return cross_entropy"
      ],
      "metadata": {
        "id": "6-YUUToDIqb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt"
      ],
      "metadata": {
        "id": "zpAFxOTGJwUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mistral"
      ],
      "metadata": {
        "id": "FiJoFG25YGpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this prompt for zero-shot classification with the Mistral model."
      ],
      "metadata": {
        "id": "3lWlAiRaKAjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_prompt = \"\"\"Classify the sentiment expressed towards the given aspect within the provided sentence as 'negative', 'neutral' or 'positive'.\n",
        "\n",
        "### Sentence:\n",
        "{}\n",
        "\n",
        "### Aspect:\n",
        "{}\n",
        "\n",
        "### Sentiment:\n",
        " \"\"\"\n",
        "# Here is a space to get the correct class label tokens\n",
        "\n",
        "def format_zero_shot_prompts(data):\n",
        "    \"\"\"\n",
        "    This function formats the prompts for zero-shot classification.\n",
        "    \"\"\"\n",
        "\n",
        "    sentences      = data[\"sentence\"]\n",
        "    aspects        = data[\"aspect_term_category\"]\n",
        "    prompts = []\n",
        "\n",
        "    for sentence, aspect in zip(sentences, aspects):\n",
        "      prompt = zero_shot_prompt.format(sentence, aspect)\n",
        "      prompts.append(prompt)\n",
        "\n",
        "    return { \"prompt\" : prompts }"
      ],
      "metadata": {
        "id": "mBbtmED0JoJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Llama and Gemma"
      ],
      "metadata": {
        "id": "sR1WOiAbYI7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this prompt for zero-shot classification with the Llama and Gemma model."
      ],
      "metadata": {
        "id": "qAC3ANHnJ7Li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_prompt = \"\"\"Classify the sentiment expressed towards the given aspect within the provided sentence as 'negative', 'neutral' or 'positive'.\n",
        "\n",
        "### Sentence:\n",
        "{}\n",
        "\n",
        "### Aspect:\n",
        "{}\n",
        "\n",
        "### Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "def format_zero_shot_prompts(data):\n",
        "    \"\"\"\n",
        "    This function formats the prompts for zero-shot classification.\n",
        "    \"\"\"\n",
        "\n",
        "    sentences      = data[\"sentence\"]\n",
        "    aspects        = data[\"aspect_term_category\"]\n",
        "    prompts = []\n",
        "\n",
        "    for sentence, aspect in zip(sentences, aspects):\n",
        "      prompt = zero_shot_prompt.format(sentence, aspect)\n",
        "      prompts.append(prompt)\n",
        "\n",
        "    return { \"prompt\" : prompts }"
      ],
      "metadata": {
        "id": "cJfHyw_KJyvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "OTRdibn1gkdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run this code, first upload the files:\n",
        "\n",
        "*   '2015_Restaurants_Train.xml'\n",
        "*   '2016_Restaurants_Train.xml'\n",
        "\n",
        "Once you have computed the cross-entropy for an LLM, start the runtime again before moving on to another LLM to prevent memory shortage."
      ],
      "metadata": {
        "id": "r8aIZn12Qk5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the 2015 data and format the prompts\n",
        "df_2015, y_2015 = load_data('2015_Restaurants_Train.xml')\n",
        "dataset_2015 = Dataset.from_pandas(df_2015)\n",
        "dataset_2015 = dataset_2015.map(format_zero_shot_prompts, batched=True,)\n",
        "\n",
        "# Load the 2016 data and format the prompts\n",
        "df_2016, y_2016 = load_data('2016_Restaurants_Train.xml')\n",
        "dataset_2016 = Dataset.from_pandas(df_2016)\n",
        "dataset_2016 = dataset_2016.map(format_zero_shot_prompts, batched=True,)"
      ],
      "metadata": {
        "id": "qlQD-8HMmPmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mistral"
      ],
      "metadata": {
        "id": "lt6CtWLPGOf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Mistral model\n",
        "model_name = \"unsloth/mistral-7b-v0.3-bnb-4bit\"\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "label_to_idx = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")"
      ],
      "metadata": {
        "id": "wmsJYi3nGTSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and save the cross-entropy complexity scores for both datasets\n",
        "ce_mistral_2015 = compute_cross_entropy(dataset_2015)\n",
        "ce_mistral_2016 = compute_cross_entropy(dataset_2016)\n",
        "\n",
        "pd.Series(ce_mistral_2015).to_csv('ce_mistral_2015.csv', index=False)\n",
        "pd.Series(ce_mistral_2016).to_csv('ce_mistral_2016.csv', index=False)"
      ],
      "metadata": {
        "id": "pvCqvD5KmWLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLaMA"
      ],
      "metadata": {
        "id": "c1tOW6YRGLZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the LLaMA model\n",
        "model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")"
      ],
      "metadata": {
        "id": "hge9Lk8sGNnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and save the cross-entropy complexity scores for both datasets\n",
        "ce_llama_2015 = compute_cross_entropy(dataset_2015)\n",
        "ce_llama_2016 = compute_cross_entropy(dataset_2016)\n",
        "\n",
        "pd.Series(ce_llama_2015).to_csv('ce_llama_2015.csv', index=False)\n",
        "pd.Series(ce_llama_2016).to_csv('ce_llama_2016.csv', index=False)"
      ],
      "metadata": {
        "id": "pcwhscd9lm6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemma"
      ],
      "metadata": {
        "id": "HdAiBKDdGQvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Gemma model\n",
        "model_name = \"unsloth/gemma-2-9b-bnb-4bit\"\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")"
      ],
      "metadata": {
        "id": "pRlAq2PZGUF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and save the cross-entropy complexity scores for both datasets\n",
        "ce_gemma_2015 = compute_cross_entropy(dataset_2015)\n",
        "ce_gemma_2016 = compute_cross_entropy(dataset_2016)\n",
        "\n",
        "pd.Series(ce_gemma_2015).to_csv('ce_gemma_2015.csv', index=False)\n",
        "pd.Series(ce_gemma_2016).to_csv('ce_gemma_2016.csv', index=False)"
      ],
      "metadata": {
        "id": "zq93FDGnmZVC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}