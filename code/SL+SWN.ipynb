{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook computes the SL and SWN complexity scores.\n",
        "\n",
        "This notebook is inspired by:\n",
        "\n",
        "Lange, N. and Frasincar, F. (2020). Curriculum learning for a hybrid approach for aspect-based sentiment\n",
        "analysis. Bachelorâ€™s thesis.\n",
        "https://github.com/NanaLange/CL-HAABSA"
      ],
      "metadata": {
        "id": "eNLlZZSvMs4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Packages"
      ],
      "metadata": {
        "id": "UE1LCCl1XsUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xml.etree.ElementTree as ET\n",
        "import nltk\n",
        "from nltk.wsd import lesk\n",
        "from nltk.corpus import sentiwordnet as sw\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import random\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "yPDS7pLuVXca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "fk1jWZxxLHkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "wzb00e7WXwU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text):\n",
        "    \"\"\"\n",
        "    Tokenize strings, only retaining alphanumeric characters.\n",
        "    \"\"\"\n",
        "\n",
        "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "    return tokenizer.tokenize(text)\n",
        "\n",
        "def count_aspect_words(target):\n",
        "    \"\"\"\n",
        "    Count the number of words in the aspect term.\n",
        "    \"\"\"\n",
        "\n",
        "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "    if not target or target == 'NULL':\n",
        "      return 0\n",
        "    return len(tokenizer.tokenize(target))\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load and preprocess the XML file into a pandas DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load and parse XML file\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Extract data into a list of dictionaries\n",
        "    data = []\n",
        "\n",
        "    # Iterate through the XML structure and extract information from the reviews\n",
        "    for review in root.findall('Review'):\n",
        "        review_id = review.get('rid')\n",
        "\n",
        "        for sentence in review.findall('.//sentence'):\n",
        "            text = sentence.find('text').text\n",
        "            sentence_length = len(tokenize_text(text))\n",
        "            opinions = sentence.findall('.//Opinion')\n",
        "            num_aspects = len(opinions)\n",
        "\n",
        "            for opinion in sentence.findall('.//Opinion'):\n",
        "                category = opinion.get('category')\n",
        "                if category == 'FOOD#GENERAL':\n",
        "                    category = 'FOOD#STYLE_OPTIONS'\n",
        "\n",
        "                data.append({\n",
        "                    \"sentence\": text,\n",
        "                    \"aspect\": opinion.get('target'),\n",
        "                    \"category\": category,\n",
        "                    \"sentiment\": opinion.get('polarity'),\n",
        "                    \"num_aspects\": num_aspects,\n",
        "                    \"tokenized_sentence\": tokenize_text(text),\n",
        "                    \"sentence_length\": sentence_length,\n",
        "                    \"num_context_words\": sentence_length - count_aspect_words(opinion.get('target'))\n",
        "                })\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    y = pd.get_dummies(df['sentiment']).astype(int).values\n",
        "\n",
        "    # Return both the features DataFrame and the sentiment labels\n",
        "    return df, y"
      ],
      "metadata": {
        "id": "2yWKtARCMCM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve SentiWordNet Features"
      ],
      "metadata": {
        "id": "Ql9Xt0vNYBlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_swn_scores(tokenized_sentence):\n",
        "    \"\"\"\n",
        "    Get SentiWordNet scores for a given (tokenized) sentence.\n",
        "    We return the Positivity (P), Negativity (N), Objectivity (O), and Absolute Difference scores (AD).\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the POS-tag for every token\n",
        "    tag = nltk.pos_tag(tokenized_sentence)\n",
        "    synset = []\n",
        "\n",
        "    # Loop through every word and POS-tag and append correct synset to the list for every word\n",
        "    for word, pos in tag:\n",
        "\n",
        "      # Map the NLTK tags to SentiWordNet tags and use LESK for word sense disambiguation to find correct synset\n",
        "      if pos.startswith('J'): # Adjective\n",
        "        pos = 'a'\n",
        "        if lesk(tokenized_sentence,word,pos) is None: # Satellite Adjective\n",
        "          pos = 's'\n",
        "        synset.append(lesk(tokenized_sentence, word, pos = pos))\n",
        "      elif pos.startswith(\"R\"): # Adverb\n",
        "        pos = 'r'\n",
        "        synset.append(lesk(tokenized_sentence, word, pos = pos))\n",
        "      elif pos.startswith(\"N\"): # Noun\n",
        "        pos = 'n'\n",
        "        synset.append(lesk(tokenized_sentence, word, pos = pos))\n",
        "      elif pos.startswith(\"V\"): # Verb\n",
        "        pos = 'v'\n",
        "        synset.append(lesk(tokenized_sentence, word, pos = pos))\n",
        "      else:\n",
        "        synset.append(lesk(tokenized_sentence, word))\n",
        "\n",
        "    # Compute the SWN features for the sentence\n",
        "    pos_score, neg_score, obj_score = 0, 0, 0\n",
        "    for syn in synset:\n",
        "      if syn is not None:\n",
        "          # Get the sentiment scores for the given synset\n",
        "          scores = sw.senti_synset(syn.name())\n",
        "          pos_score += scores.pos_score()\n",
        "          neg_score += scores.neg_score()\n",
        "          obj_score += scores.obj_score()\n",
        "    abs_diff_score = abs(pos_score - neg_score)\n",
        "\n",
        "    return pos_score, neg_score, obj_score, abs_diff_score"
      ],
      "metadata": {
        "id": "MxGQJlx1wTEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get feature matrix Dataframe"
      ],
      "metadata": {
        "id": "AdHziK-F4TC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate features based on the tokenized sentences and other extracted information\n",
        "def extract_features(df):\n",
        "    \"\"\"\n",
        "    Construct the feature matrix which we use as input for the Auxiliary feedforward neural network.\n",
        "    \"\"\"\n",
        "    df_features = pd.DataFrame()\n",
        "\n",
        "    # Sentence Length\n",
        "    df_features['l'] = df['sentence_length'].values\n",
        "\n",
        "    # SentiWordNet features (Positivity, Negativity, Objectivity, and Absolute Difference scores)\n",
        "    df_features[['P', 'N', 'O', 'AD']] = df['tokenized_sentence'].apply(\n",
        "        lambda x: pd.Series(get_swn_scores(x))\n",
        "    )\n",
        "\n",
        "    # Number of Aspects\n",
        "    df_features['A'] = df['num_aspects'].values\n",
        "\n",
        "    # Number of Context Words\n",
        "    df_features['W'] = df['num_context_words'].values\n",
        "\n",
        "    # Scaled versions of the abovementioned features\n",
        "    for feature in ['P', 'N', 'O', 'AD', 'A', 'W']:\n",
        "        df_features[f'{feature}_scaled'] = df_features[feature] / df_features['l']\n",
        "\n",
        "    # Save the columns to normalize\n",
        "    columns_to_normalize = df_features.columns\n",
        "\n",
        "    # One-hot encoded categories\n",
        "    one_hot_encoded = pd.get_dummies(df['category']).astype(int)\n",
        "    df_features = pd.concat([df_features, one_hot_encoded], axis=1)\n",
        "\n",
        "    # Normalize all features (except categories) to lie in the range [0,1]\n",
        "    scaler = MinMaxScaler()\n",
        "    df_features[columns_to_normalize] = scaler.fit_transform(df_features[columns_to_normalize])\n",
        "\n",
        "    # Return the final DataFrame with all features\n",
        "    return df_features"
      ],
      "metadata": {
        "id": "tVONtFpVRJB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auxiliary feedforward Neural Network"
      ],
      "metadata": {
        "id": "eRroQUZWIQAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def aux_model(df_train, y_train, df_test, y_test):\n",
        "    \"\"\"\n",
        "    Run the auxiliary feedforward neural network to obtain the complexity scores for every training instance.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the feature matrices\n",
        "    X_train = df_train.values\n",
        "    X_test = df_test.values\n",
        "\n",
        "    # Build the neural network\n",
        "    model = Sequential()\n",
        "    model.add(Dense(183, input_dim=X_train.shape[1], activation='relu'))\n",
        "    model.add(Dropout(0.6))\n",
        "    model.add(Dense(140, activation='relu', kernel_initializer=tf.keras.initializers.GlorotUniform(), bias_initializer=tf.keras.initializers.Zeros()))\n",
        "    model.add(Dropout(0.6))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "    # Train the model on the training data\n",
        "    history = model.fit(X_train, y_train, epochs=600, batch_size=198, verbose=0)\n",
        "\n",
        "    # Compute the prediction probabilities, predicted class, and true class\n",
        "    prob_y_train = model.predict(X_train)\n",
        "    pred_y_train = np.argmax(prob_y_train, axis=1)\n",
        "    y_train_labels = np.argmax(y_train,axis=1)\n",
        "\n",
        "    # Compute the in-sample accuracy\n",
        "    train_acc = accuracy_score(pred_y_train, y_train_labels)\n",
        "    print('Train accuracy is: ', train_acc * 100)\n",
        "\n",
        "    # Compute the out-of-sample accuracy\n",
        "    prob_y_test = model.predict(X_test)\n",
        "    pred_y_test = np.argmax(prob_y_test, axis=1)\n",
        "    y_test_labels = np.argmax(y_test,axis=1)\n",
        "    test_acc = accuracy_score(pred_y_test, y_test_labels)\n",
        "    print('Test accuracy is: ', test_acc * 100)\n",
        "\n",
        "    # Compute the complexity scores\n",
        "    complexity_scores = []\n",
        "    for prob, label in zip(prob_y_train, y_train):\n",
        "      score = np.sum(np.square(np.subtract(prob, label)))\n",
        "      complexity_scores.append(score)\n",
        "\n",
        "    return complexity_scores\n"
      ],
      "metadata": {
        "id": "k-nqe_4VI2PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def averaged_curriculum_scores(df_train, y_train, df_test, y_test, num_runs):\n",
        "    \"\"\"\n",
        "    #Run aux model num_runs times and get averaged curriculum scores to mitigate initialisation sensitivity\n",
        "    \"\"\"\n",
        "    scores_list = []\n",
        "\n",
        "    for i in range(num_runs):\n",
        "      random.seed(i)\n",
        "      np.random.seed(i)\n",
        "      tf.random.set_seed(i)\n",
        "\n",
        "      scores = aux_model(df_train, y_train, df_test, y_test)\n",
        "      scores_list.append(scores)\n",
        "\n",
        "    averaged_scores = np.mean(scores_list, axis=0)\n",
        "    return averaged_scores"
      ],
      "metadata": {
        "id": "GyEICCz7hz9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "4Z_IYW91TL81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Files"
      ],
      "metadata": {
        "id": "2e46773WLlst"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run this code, first upload the files:\n",
        "\n",
        "*   '2015_Restaurants_Train.xml'\n",
        "*   '2015_Restaurants_Test.xml'\n",
        "*   '2016_Restaurants_Train.xml'\n",
        "*   '2016_Restaurants_Test.xml'"
      ],
      "metadata": {
        "id": "im1RTDl5LiRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2015"
      ],
      "metadata": {
        "id": "PWuqxMxZLcLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load 2015 train and test data\n",
        "df_train_2015, y_train_2015 = load_data(\"2015_Restaurants_Train.xml\")\n",
        "df_feature_train_2015 = extract_features(df_train_2015)\n",
        "\n",
        "df_test_2015, y_test_2015 = load_data(\"2015_Restaurants_Test.xml\")\n",
        "df_feature_test_2015 = extract_features(df_test_2015)"
      ],
      "metadata": {
        "id": "skQxiCmWZbPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get SL and SWN complexity scores\n",
        "sentence_lengths_2015 = df_feature_train_2015['l'].values\n",
        "swn_complexity_scores_2015 = averaged_curriculum_scores(df_feature_train_2015, y_train_2015, df_feature_test_2015, y_test_2015, num_runs = 10)\n",
        "\n",
        "# Save the complexity scores to csv files\n",
        "pd.Series(sentence_lengths_2015).to_csv('sentence_lengths_2015.csv', index=False)\n",
        "pd.Series(swn_complexity_scores_2015).to_csv('swn_complexity_scores_2015.csv', index=False)"
      ],
      "metadata": {
        "id": "0bwM4HMzL3Hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2016"
      ],
      "metadata": {
        "id": "aEl9ySThL0By"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load 2016 train and test data\n",
        "df_train_2016, y_train_2016 = load_data(\"2016_Restaurants_Train.xml\")\n",
        "df_feature_train_2016 = extract_features(df_train_2016)\n",
        "\n",
        "df_test_2016, y_test_2016 = load_data(\"2016_Restaurants_Test.xml\")\n",
        "df_feature_test_2016 = extract_features(df_test_2016)"
      ],
      "metadata": {
        "id": "IvVo0HtKdAxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get SL and SWN complexity scores\n",
        "sentence_lengths_2016 = df_feature_train_2016['l'].values\n",
        "swn_complexity_scores_2016 = averaged_curriculum_scores(df_feature_train_2016, y_train_2016, df_feature_test_2016, y_test_2016,10)\n",
        "\n",
        "# Save the complexity scores to csv files\n",
        "pd.Series(sentence_lengths_2016).to_csv('sentence_lengths_2016.csv', index=False)\n",
        "pd.Series(swn_complexity_scores_2016).to_csv('swn_complexity_scores_2016.csv', index=False)"
      ],
      "metadata": {
        "id": "t4cCp51sguhI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}