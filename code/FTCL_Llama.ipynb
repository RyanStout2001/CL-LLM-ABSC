{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook fine-tunes Llama-3.1-8B model using either standard fine-tuning or fine-tuning through curriculum learning with BBS. To fine-tune the model we use QLORA with 4bit quantization. This notebook contributes to the results shown in Section 5.2.2.\n",
        "\n",
        "This notebook is inspired by: https://github.com/unslothai/unsloth"
      ],
      "metadata": {
        "id": "rl7aUryTtuC3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqAfMipEb40W"
      },
      "source": [
        "# Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBfYy7f2HiX2"
      },
      "outputs": [],
      "source": [
        "%%writefile requirements.txt\n",
        "unsloth==2025.6.2\n",
        "unsloth_zoo==2025.6.1\n",
        "trl==0.15.2\n",
        "xformers==0.0.29.post3\n",
        "bitsandbytes==0.46.0\n",
        "peft==0.15.2\n",
        "accelerate==1.7.0\n",
        "torch==2.6.0+cu124\n",
        "transformers==4.52.4\n",
        "datasets==3.6.0\n",
        "pandas==2.2.2\n",
        "scikit-learn==1.6.1\n",
        "sentencepiece==0.2.0\n",
        "huggingface-hub\n",
        "hf_transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BD9bFHbIHpG4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth import is_bfloat16_supported\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from datasets import Dataset, concatenate_datasets\n",
        "import xml.etree.ElementTree as ET\n",
        "import json\n",
        "import ast\n",
        "from tqdm.auto import tqdm\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, EarlyStoppingCallback\n",
        "from transformers import DataCollatorForLanguageModeling, DataCollatorWithPadding\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "import transformers.utils.logging\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed for remainder of the code\n",
        "seed_val = 546297\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed_val)"
      ],
      "metadata": {
        "id": "7slpqqY8RiaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXG1VxoAb8Hs"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RM55EcBzJi5"
      },
      "source": [
        "Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3WI_E84Hql_"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load and preprocess the XML file into a pandas DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load and parse XML file\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Extract data into a list of dictionaries\n",
        "    data = []\n",
        "\n",
        "    # Iterate through the XML structure and extract infomation from the reviews\n",
        "    for review in root.findall('Review'):\n",
        "      review_id = review.get('rid')\n",
        "\n",
        "      for sentence in review.findall('.//sentence'):\n",
        "        text = sentence.find('text').text\n",
        "\n",
        "        # Show nothing rather than 'NULL' in the prompt\n",
        "        for opinion in sentence.findall('.//Opinion'):\n",
        "          aspect = opinion.get('target')\n",
        "          if aspect == 'NULL':\n",
        "            aspect = ''\n",
        "\n",
        "          # Adjust the categories to be more informative for the prompt\n",
        "          category = opinion.get('category').lower().replace('#', ' ').replace('_',' and ')\n",
        "          if category == 'food general':\n",
        "              category = 'food style and options'\n",
        "          elif category == 'service general':\n",
        "              category = 'service'\n",
        "          elif category == 'restaurant general' or category == 'restaurant miscellaneous':\n",
        "              category = 'restaurant'\n",
        "          elif category == 'ambience general':\n",
        "              category = 'ambience'\n",
        "          elif category == 'location general':\n",
        "              category = 'location'\n",
        "\n",
        "          # Represent the aspect as 'term (category entity)'\n",
        "          aspect_term_category = aspect + ' (' + category + ')'\n",
        "\n",
        "          data.append({\n",
        "              \"sentence\": text,\n",
        "              \"aspect\": aspect,\n",
        "              \"category\": category,\n",
        "              \"aspect_term_category\": aspect_term_category,\n",
        "              \"sentiment\": opinion.get('polarity')\n",
        "          })\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    y = df['sentiment'].values\n",
        "    label_to_idx = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "    df['sentiment_label'] = df['sentiment'].map(label_to_idx)\n",
        "\n",
        "    # Return both the features DataFrame and the sentiment labels\n",
        "    return df, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6rERs-eSr6b"
      },
      "outputs": [],
      "source": [
        "def zero_shot_classification(dataset):\n",
        "    \"\"\"\n",
        "    Perform zero-shot classification by predicting the most likely sentiment.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the classes and get their token IDS\n",
        "    classes = [\"negative\", \"neutral\", \"positive\"]\n",
        "    class_token_ids = [tokenizer.encode(c, add_special_tokens=False)[0] for c in classes]\n",
        "\n",
        "    # Create lists to store the predicted classes\n",
        "    pred_sent = []\n",
        "\n",
        "    # Iterate over the dataset\n",
        "    for row in tqdm(dataset):\n",
        "      input = row['zero_shot_prompt']\n",
        "      inputs = tokenizer(input, return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "      # Perform inference without computing gradients to save memory\n",
        "      with torch.no_grad():\n",
        "        # This returns the outputs for all input tokens and the next one\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "      # The predicted class should be at the last token (= logit.size(1) -1)\n",
        "      logits = outputs.logits\n",
        "      prediction_position = logits.size(1) - 1\n",
        "\n",
        "      # Retrieve the logits for the classes at the specified position (end of prompt)\n",
        "      logits_for_prediction = logits[:, prediction_position, :]\n",
        "      class_logits = logits_for_prediction[:, class_token_ids]\n",
        "\n",
        "      # Obtain the predicted probabilities class by using the softmax and argmax function respectively\n",
        "      class_probs = F.softmax(class_logits, dim=-1)\n",
        "      probs = class_probs.cpu().detach().float().numpy()\n",
        "      pred_sent.append(classes[np.argmax(probs)])\n",
        "\n",
        "      # To save memory\n",
        "      del inputs, outputs, logits, logits_for_prediction, class_logits, class_probs\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    return pred_sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UBbdJClzgmm"
      },
      "outputs": [],
      "source": [
        "class DataCollatorForLabelOnlyLoss(DataCollatorForLanguageModeling):\n",
        "    \"\"\"\n",
        "    A custom data collator to compute the loss function on the sentiment label only.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, label_texts, ignore_index=-100, *args, **kwargs):\n",
        "        super().__init__(tokenizer=tokenizer, mlm=False, *args, **kwargs)\n",
        "\n",
        "        # Setting the label to -100 makes the model ignore it\n",
        "        self.ignore_index = ignore_index\n",
        "        # Store the token IDs for the sentiment labels in a dictionary\n",
        "        self.label_token_ids = {label: tokenizer.encode(label, add_special_tokens=False) for label in label_texts}\n",
        "\n",
        "    def torch_call(self, examples):\n",
        "\n",
        "        # Examples is a list of (tokenized) single data instances\n",
        "        batch = super().torch_call(examples)\n",
        "\n",
        "        # Loop over every training instance\n",
        "        for i in range(len(batch[\"labels\"])):\n",
        "\n",
        "            # Find the non-padding tokens\n",
        "            non_ignored = (batch[\"labels\"][i] != self.ignore_index).nonzero(as_tuple=True)[0]\n",
        "\n",
        "            # The sentiment prediction is at the penultimate non-padding token (EOS token is last one)\n",
        "            label_pos = non_ignored[-2].item()\n",
        "\n",
        "            # Mask everything except the label and the EOS-token\n",
        "            batch[\"labels\"][i][:label_pos] = self.ignore_index\n",
        "            batch[\"labels\"][i][label_pos+1:] = self.ignore_index\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To prevent redundant errors from being printed\n",
        "transformers.utils.logging.get_logger(\"transformers.trainer\").setLevel(\"ERROR\")\n",
        "\n",
        "class CustomSFTTrainer(SFTTrainer):\n",
        "    \"\"\"\n",
        "    A custom SFTTrainer that allows us to compute the accuracy, f1-score and convergence metric of the validation set during training.\n",
        "    This allows us to monitor the performance convergence of the model during training.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        # Store the token IDs for the sentiment classes\n",
        "        self.class_token_ids = []\n",
        "        for c in CLASSES:\n",
        "            token_ids = self.tokenizer.encode(c, add_special_tokens=False)\n",
        "            self.class_token_ids.append(token_ids[0])\n",
        "\n",
        "    def get_eval_dataloader(self, eval_dataset: Dataset = None) -> DataLoader:\n",
        "        \"\"\"\n",
        "        Override the default dataloader to manually tokenize the input and format the data for the custom prediction_step function.\n",
        "        \"\"\"\n",
        "\n",
        "        # Use the provided evaluation dataset, or fall back to the one defined in the Trainer.\n",
        "        current_eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
        "\n",
        "        # Extract a list of the prompts (\"text\") and labels in the eval set and tokenize the prompts\n",
        "        texts = current_eval_dataset[\"text\"]\n",
        "        metric_labels = current_eval_dataset[\"labels\"]\n",
        "        tokenized_inputs = self.tokenizer(texts, truncation=True, max_length=max_seq_length, padding=False)\n",
        "\n",
        "        # Reconstruct the tokenized data from a columnar format (dict of lists) to a row format (list of dicts), where each dict is a single example.\n",
        "        # This has to be done to format the data in the way the function prediction_step expects it\n",
        "        processed_examples = []\n",
        "        for i in range(len(texts)):\n",
        "            example = {}\n",
        "            for key in tokenized_inputs.keys():\n",
        "                example[key] = tokenized_inputs[key][i]\n",
        "            example[\"labels\"] = metric_labels[i]\n",
        "            processed_examples.append(example)\n",
        "\n",
        "        # Convert back to a huggingface dataset\n",
        "        tokenized_eval_dataset = Dataset.from_list(processed_examples)\n",
        "\n",
        "        # Initialize the new dataloader with our custom dataset\n",
        "        eval_loader = DataLoader(\n",
        "            tokenized_eval_dataset,\n",
        "            batch_size=self.args.eval_batch_size,\n",
        "            collate_fn=DataCollatorWithPadding(self.tokenizer, padding=\"longest\"),\n",
        "            drop_last=self.args.dataloader_drop_last,\n",
        "            num_workers=self.args.dataloader_num_workers,\n",
        "            pin_memory=self.args.dataloader_pin_memory,\n",
        "        )\n",
        "\n",
        "        return eval_loader\n",
        "\n",
        "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
        "        \"\"\"\n",
        "        A custom prediction_step function that returns the predicted and true classes for every isntance, allowing us to compute our custom evaluation metrics.\n",
        "        The default prediction_step copmuted the predicted the logits of all tokens, rather than all tokens, and returned the evaluation loss, which we are less interested in.\n",
        "        \"\"\"\n",
        "\n",
        "        # Move the input tensors from the dataloader batch to the correct GPU device\n",
        "        input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "        attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
        "\n",
        "        # Safely extract the ground truth integer labels from the batch, regardless of the shape\n",
        "        true_metric_labels = torch.full((input_ids.shape[0],), -100, device=model.device, dtype=torch.long)\n",
        "        if \"labels\" in inputs:\n",
        "            labels_from_input = inputs[\"labels\"].to(model.device)\n",
        "            if labels_from_input.ndim == 1:\n",
        "                true_metric_labels = labels_from_input\n",
        "            elif labels_from_input.ndim == 2 and labels_from_input.shape[1] == 1: # Squeeze if (batch_size, 1)\n",
        "                true_metric_labels = labels_from_input.squeeze(1)\n",
        "\n",
        "        # Perform inference without computing gradients to save memory\n",
        "        with torch.no_grad():\n",
        "            # This returns the outputs for all input tokens and the next one\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        # Get the number of examples in current batch\n",
        "        batch_size = input_ids.shape[0]\n",
        "\n",
        "        # Copmute the length of each sequence in the batch, ignoring any padding tokens\n",
        "        if self.tokenizer.pad_token_id is None:\n",
        "            sequence_lengths = torch.ne(input_ids, 0).sum(-1)\n",
        "        else:\n",
        "            sequence_lengths = torch.ne(input_ids, self.tokenizer.pad_token_id).sum(-1)\n",
        "\n",
        "        # Retrieve the index of the last token of the prompt, and the logits of the next index, the predicted sentiment (comes after last prompt token)\n",
        "        indices_of_last_prompt_token = sequence_lengths - 1\n",
        "        next_token_prediction_logits = logits[torch.arange(batch_size, device=model.device), indices_of_last_prompt_token, : ]\n",
        "\n",
        "        # Compute the predicted class given the logits of the sentiment labels\n",
        "        class_specific_logits = next_token_prediction_logits[:, self.class_token_ids]\n",
        "        preds_class_indices = torch.argmax(class_specific_logits, dim=-1)\n",
        "\n",
        "        return (None, preds_class_indices, true_metric_labels)\n"
      ],
      "metadata": {
        "id": "6X6DnONxu2aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_eval_metrics(eval_preds: EvalPrediction):\n",
        "    \"\"\"\n",
        "    Compute the accuracy, macro f1-score, weighted f1-score, and convergence metric of the validation set.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the indices predicted and true classes\n",
        "    true_classes = eval_preds.label_ids\n",
        "    predicted_classes = eval_preds.predictions\n",
        "\n",
        "    # Make sure we do not include -100 as a class, which could be added due to padding\n",
        "    valid_mask = true_classes != -100\n",
        "    filtered_true = true_classes[valid_mask]\n",
        "    filtered_preds = predicted_classes[valid_mask]\n",
        "\n",
        "    # Compute the evaluation metrics\n",
        "    accuracy = accuracy_score(filtered_true, filtered_preds)\n",
        "    f1_macro = f1_score(filtered_true, filtered_preds, average='macro', zero_division=0)\n",
        "    f1_weighted = f1_score(filtered_true, filtered_preds, average='weighted', zero_division=0)\n",
        "\n",
        "    # Create a dictionary with the obtained metrcis\n",
        "    metrics_dict = {\n",
        "        \"eval_accuracy\": accuracy,\n",
        "        \"eval_f1_macro\": f1_macro,\n",
        "        \"eval_f1_weighted\": f1_weighted,\n",
        "        \"convergence_metric\": accuracy + 0.001*f1_macro\n",
        "    }\n",
        "\n",
        "    return metrics_dict"
      ],
      "metadata": {
        "id": "sX-63ZS6tx_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def BS(dataset, num_subsets, cur_order):\n",
        "    \"\"\"\n",
        "    This function partitions the data into buckets to perform curriculum learning with BS\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the list of subsets and indices\n",
        "    subsets = []\n",
        "    sorted_data = dataset.sort(cur_order)\n",
        "    end_index = 0\n",
        "    subset_size = int(np.ceil(len(sorted_data)/num_subsets))\n",
        "\n",
        "    # Create the buckets one by one\n",
        "    for i in range(num_subsets):\n",
        "      if i < num_subsets - 1:\n",
        "        end_index += subset_size\n",
        "      else:\n",
        "        end_index = len(sorted_data)\n",
        "      subset = sorted_data.select(range(0, end_index))\n",
        "      subsets.append(subset)\n",
        "\n",
        "    return subsets"
      ],
      "metadata": {
        "id": "TUMwf_9e1JzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def BBS(dataset, num_subsets, cur_order):\n",
        "    \"\"\"\n",
        "    This function partitions the data into proportional buckets to perform curriculum learning with BBS\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the list of subsets and indices\n",
        "    subsets = []\n",
        "    negative_end_index = 0\n",
        "    neutral_end_index = 0\n",
        "    positive_end_index = 0\n",
        "\n",
        "    # Partition the data based on their sentimeent classes\n",
        "    negative_data = dataset.filter(lambda example: example['sentiment'] == 'negative').sort(cur_order)\n",
        "    neutral_data = dataset.filter(lambda example: example['sentiment'] == 'neutral').sort(cur_order)\n",
        "    positive_data = dataset.filter(lambda example: example['sentiment'] == 'positive').sort(cur_order)\n",
        "\n",
        "    # Determine the number of data instances of each class are in the subsets to maintain a class distribution proportional to the full dataset.\n",
        "    negative_subset_size = int(np.ceil(len(negative_data)/num_subsets))\n",
        "    neutral_subset_size = int(np.ceil(len(neutral_data)/num_subsets))\n",
        "    positive_subset_size = int(np.ceil(len(positive_data)/num_subsets))\n",
        "\n",
        "    # Create the buckets one by one\n",
        "    for i in range(0,num_subsets):\n",
        "\n",
        "      # The number of data instances of each class should be in the buckets\n",
        "      negative_end_index += negative_subset_size\n",
        "      neutral_end_index += neutral_subset_size\n",
        "      positive_end_index += positive_subset_size\n",
        "\n",
        "      # Create the buckets by concatenating the subsets of the different classes back together\n",
        "      if i < num_subsets - 1:\n",
        "        subset = concatenate_datasets([negative_data.select(range(0, negative_end_index)), neutral_data.select(range(0, neutral_end_index)), positive_data.select(range(0, positive_end_index))]).shuffle(seed = i)\n",
        "      else:\n",
        "        subset = concatenate_datasets([negative_data.select(range(0, len(negative_data))), neutral_data.select(range(0, len(neutral_data))), positive_data.select(range(0, len(positive_data)))]).shuffle(seed = i)\n",
        "      subsets.append(subset)\n",
        "\n",
        "    return subsets"
      ],
      "metadata": {
        "id": "KQJc77tMRRYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y52qQGjS-CQf"
      },
      "source": [
        "# Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prompt used for zero-shot classification"
      ],
      "metadata": {
        "id": "Z1fBThxVYtiL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJt7TEud8I1R"
      },
      "outputs": [],
      "source": [
        "zero_shot_prompt = \"\"\"Classify the sentiment expressed towards the given aspect within the provided sentence as 'negative', 'neutral' or 'positive'.\n",
        "\n",
        "### Sentence:\n",
        "{}\n",
        "\n",
        "### Aspect:\n",
        "{}\n",
        "\n",
        "### Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "def format_zero_shot_prompts(data):\n",
        "    \"\"\"\n",
        "    This function formats the prompts for zero-shot classification.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the sentences and aspects from the data\n",
        "    sentences      = data[\"sentence\"]\n",
        "    aspects        = data[\"aspect_term_category\"]\n",
        "    prompts = []\n",
        "\n",
        "    # Format the zero-shot prompts for every test instance\n",
        "    for sentence, aspect in zip(sentences, aspects):\n",
        "      prompt = zero_shot_prompt.format(sentence, aspect)\n",
        "      prompts.append(prompt)\n",
        "\n",
        "    return { \"zero_shot_prompt\" : prompts }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prompt used for model evaluation during training"
      ],
      "metadata": {
        "id": "7qEvZZDGYv86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_prompt_template = \"\"\"Classify the sentiment expressed towards the given aspect within the provided sentence as 'negative', 'neutral' or 'positive'.\n",
        "\n",
        "### Sentence:\n",
        "{}\n",
        "\n",
        "### Aspect:\n",
        "{}\n",
        "\n",
        "### Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "CLASSES = [\"negative\", \"neutral\", \"positive\"]\n",
        "word_to_id = {text: i for i, text in enumerate(CLASSES)}\n",
        "\n",
        "def format_eval_prompt(data):\n",
        "    \"\"\"\n",
        "    This function prepares the data from the validation set for the SFFTrainer, by formatting the prompts and\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the sentences, aspects, and sentiments from the data\n",
        "    sentences      = data[\"sentence\"]\n",
        "    aspects        = data[\"aspect_term_category\"]\n",
        "    sentiments     = data[\"sentiment\"]\n",
        "    prompts = []\n",
        "    true_class_indices = []\n",
        "\n",
        "    for sentence, aspect, sentiment in zip(sentences, aspects, sentiments):\n",
        "\n",
        "        # Format the prompt\n",
        "        prompt_text = eval_prompt_template.format(sentence, aspect)\n",
        "        prompts.append(prompt_text)\n",
        "\n",
        "        # Convert true sentiment word to class index\n",
        "        class_idx = word_to_id.get(sentiment, -1) # Default to -1 if not found\n",
        "        if class_idx == -1:\n",
        "            print(f\"Warning: Unknown sentiment word '{sentiment}' found in eval data. Assigning label -1.\")\n",
        "        true_class_indices.append(class_idx)\n",
        "\n",
        "    return {\"text\": prompts, \"labels\": true_class_indices}"
      ],
      "metadata": {
        "id": "rqOgqJKj1afV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prompt used during the fine-tuning of the model"
      ],
      "metadata": {
        "id": "WQpH-ZRAfxKr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1s73AxA-Pss"
      },
      "outputs": [],
      "source": [
        "fine_tune_prompt = \"\"\"Classify the sentiment expressed towards the given aspect within the provided sentence as 'negative', 'neutral' or 'positive'.\n",
        "\n",
        "### Sentence:\n",
        "{}\n",
        "\n",
        "### Aspect:\n",
        "{}\n",
        "\n",
        "### Sentiment:\n",
        "{}\"\"\"\n",
        "\n",
        "def format_fine_tune_prompts(data):\n",
        "    \"\"\"\n",
        "    This function formats the prompts for the fine-tuning of the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the sentences, aspects, and sentiments from the data\n",
        "    sentences      = data[\"sentence\"]\n",
        "    aspects        = data[\"aspect_term_category\"]\n",
        "    sentiments     = data[\"sentiment\"]\n",
        "    prompts = []\n",
        "    EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "    # Format the fine-tune prompts for every training instance\n",
        "    for sentence, aspect, sentiment in zip(sentences, aspects, sentiments):\n",
        "      prompt = fine_tune_prompt.format(sentence, aspect, sentiment) + EOS_TOKEN # Append EOS Token for fine-tuning\n",
        "      prompts.append(prompt)\n",
        "\n",
        "    return { \"text\" : prompts }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiS0AmrOZ-xq"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run this code, first upload the files:\n",
        "\n",
        "*   '201x_Restaurants_Train.xml'\n",
        "*   '201x_Restaurants_Test.xml'\n",
        "*   'swn_complexity_scores_201x.csv'\n",
        "*   'sentence_lengths_201x.csv'\n",
        "*   'ce_llama_201x.csv'\n",
        "\n",
        "with 201x being either 2015 or 2016.\n",
        "\n",
        "Once you have completed the fine-tuning, computed the evaluation metrics, and saved the results, reconnect the runtime before running another case (different dataset/training procedure)."
      ],
      "metadata": {
        "id": "LQToY2zAf9xr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model"
      ],
      "metadata": {
        "id": "nZm9MlZaZmij"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrYSKdhTaykl"
      },
      "source": [
        "Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWg0JaguwA2v"
      },
      "outputs": [],
      "source": [
        "model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m13dli5aBC3"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 1024\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "CLASSES = [\"negative\", \"neutral\", \"positive\"]\n",
        "#wandb.login()\n",
        "#os.environ[\"WANDB_PROJECT\"] = \"project_name\" # Specify wandb project name\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = 1024,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0.05,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 546297,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "id": "HqvOSWyGoYQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh-rYhT9a0ZC"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load either the 2015 or 2016 data, perform the train-validation split and format the prompts"
      ],
      "metadata": {
        "id": "2fRbAZuRkTzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2015"
      ],
      "metadata": {
        "id": "_K-BkycakKpm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHZ1bAvCa64v"
      },
      "outputs": [],
      "source": [
        "# Load train data\n",
        "df_train, y_train = load_data('2015_Restaurants_Train.xml')\n",
        "df_train['original_indices'] = df_train.index\n",
        "df_train['sl'] = pd.read_csv('sentence_lengths_2015.csv')\n",
        "df_train['swn'] = pd.read_csv('swn_complexity_scores_2015.csv')\n",
        "df_train['ce'] = pd.read_csv('ce_llama_2015.csv')\n",
        "\n",
        "# Load test data\n",
        "df_test, y_test = load_data('2015_Restaurants_Test.xml')\n",
        "df_test['original_indices'] = df_test.index\n",
        "\n",
        "# Convert to huggingface Datasets\n",
        "dataset_train = Dataset.from_pandas(df_train)\n",
        "dataset_test = Dataset.from_pandas(df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2016"
      ],
      "metadata": {
        "id": "4z1qmrOQkM9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train data\n",
        "df_train, y_train = load_data('2016_Restaurants_Train.xml')\n",
        "df_train['original_indices'] = df_train.index\n",
        "df_train['sl'] = pd.read_csv('sentence_lengths_2016.csv')\n",
        "df_train['swn'] = pd.read_csv('swn_complexity_scores_2016.csv')\n",
        "df_train['ce'] = pd.read_csv('ce_llama_2016.csv')\n",
        "\n",
        "# Load test data\n",
        "df_test, y_test = load_data('2016_Restaurants_Test.xml')\n",
        "df_test['original_indices'] = df_test.index\n",
        "\n",
        "# Convert to huggingface Datasets\n",
        "dataset_train = Dataset.from_pandas(df_train)\n",
        "dataset_test = Dataset.from_pandas(df_test)"
      ],
      "metadata": {
        "id": "ZSXbZ2gCZdnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split data and format the prompts"
      ],
      "metadata": {
        "id": "R8HgaNrQkSRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the train-validation split\n",
        "split = dataset_train.train_test_split(test_size=0.2, seed = 123)\n",
        "\n",
        "# Get the sentiment labels for the train, validation and test set\n",
        "y_train = split['train']['sentiment']\n",
        "y_val = split['test']['sentiment']\n",
        "y_test = df_test['sentiment']\n",
        "\n",
        "# Format the prompts for every set\n",
        "train_set = split['train'].map(format_fine_tune_prompts, batched=True, remove_columns=split['train'].column_names)\n",
        "eval_set = split['test'].map(format_eval_prompt, batched=True, remove_columns=split['test'].column_names)\n",
        "test_set = dataset_test.map(format_zero_shot_prompts, batched=True, remove_columns=dataset_test.column_names)"
      ],
      "metadata": {
        "id": "R9aWkFkO2a2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline"
      ],
      "metadata": {
        "id": "spaH94WaYej7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model with standard fine-tuning"
      ],
      "metadata": {
        "id": "BJeLwLO0mVkn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ao_Qg5YJqafi"
      },
      "outputs": [],
      "source": [
        "# Initialize the custom SFFTrainer\n",
        "trainer = CustomSFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_set,\n",
        "    eval_dataset = eval_set,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    data_collator = DataCollatorForLabelOnlyLoss(tokenizer=tokenizer, label_texts=CLASSES), # The custom data collator\n",
        "    compute_metrics = compute_eval_metrics, # The custom convergence metrics\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)], # Perform early stopping\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 10,\n",
        "        learning_rate = 2e-5,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 25,\n",
        "        eval_steps = 25,\n",
        "        eval_strategy='steps',\n",
        "        save_steps = 25,\n",
        "        save_strategy='steps',\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='convergence_metric', # Early stopping based on the convergence metric\n",
        "        greater_is_better=True,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.1,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 546297,\n",
        "        output_dir = \"output_dir\",\n",
        "        run_name=\"run_name\", # Specify wandb run name\n",
        "        report_to = \"none\", # Switch to \"wandb\" if desired\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer_stats = trainer.train()\n",
        "print(trainer_stats.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the evaluation metrics for the test, train and validaiton set"
      ],
      "metadata": {
        "id": "hA1dLvsoma8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the evaluation metrics and save the results for the test set\n",
        "y_pred_test = zero_shot_classification(test_set)\n",
        "print(classification_report(y_test, y_pred_test, digits = 4))\n",
        "pd.Series(y_pred_test).to_csv('llama2016_baseline_test.csv', index=False)"
      ],
      "metadata": {
        "id": "lEqU8QeBBtCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the evaluation metrics and save the results for the train set\n",
        "train_class = split['train'].map(format_zero_shot_prompts, batched=True,)\n",
        "y_pred_train = zero_shot_classification(train_class)\n",
        "print(classification_report(y_train, y_pred_train, digits = 4))\n",
        "pd.Series(y_pred_test).to_csv('llama2016_baseline_train.csv', index=False)"
      ],
      "metadata": {
        "id": "fE_-9lgHG4Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the evaluation metrics and save the results for the validation set\n",
        "eval_class = split['test'].map(format_zero_shot_prompts, batched=True,)\n",
        "y_pred_val = zero_shot_classification(eval_class)\n",
        "print(classification_report(y_val, y_pred_val, digits = 4))\n",
        "pd.Series(y_pred_test).to_csv('llama2016_baseline_val.csv', index=False)"
      ],
      "metadata": {
        "id": "MPhfBslzR4PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BS"
      ],
      "metadata": {
        "id": "_JZbILE31OmP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create training buckets for BS and use 'sl', 'swn', or 'ce' as a complexity measure in the last argument"
      ],
      "metadata": {
        "id": "cYKGr7iK1OmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_subsets = BS(split['train'], 3, 'ce')"
      ],
      "metadata": {
        "id": "3zxyQ9vF1OmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model with curriculum learning using BS"
      ],
      "metadata": {
        "id": "2D8qgigM1OmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the settings for every training phase, note that we train on the easy data for 1 epoch, while we train on the medium and hard data until convergence\n",
        "phase_configs = [\n",
        "    {\"name\": \"easy\", \"num_train_epochs\": 1,  \"learning_rate\": 2e-5,  \"warmup_ratio\": 0.1, \"eval_steps\": 20, \"callbacks\": [EarlyStoppingCallback(early_stopping_patience=100)], \"load_model\": False},\n",
        "    {\"name\": \"medium\", \"num_train_epochs\": 7, \"learning_rate\": 2e-5, \"warmup_ratio\": 0.06, \"eval_steps\": 25, \"callbacks\": [EarlyStoppingCallback(early_stopping_patience=5)], \"load_model\": True},\n",
        "    {\"name\": \"hard\", \"num_train_epochs\": 10, \"learning_rate\": 2e-5,\"warmup_ratio\": 0.03, \"eval_steps\": 30, \"callbacks\" : [EarlyStoppingCallback(early_stopping_patience=5)], \"load_model\": True}\n",
        "]\n",
        "\n",
        "# Iterate over the training phases\n",
        "for phase_idx, (subset, config) in enumerate(zip(train_subsets, phase_configs)):\n",
        "    print(f\"\\n=== Starting {config['name']} phase ===\")\n",
        "    train_subset = subset.map(format_fine_tune_prompts, batched=True, remove_columns=subset.column_names)\n",
        "\n",
        "    # Initialize the custom SFFTrainer for each phase\n",
        "    trainer = CustomSFTTrainer(\n",
        "        model = model,\n",
        "        tokenizer = tokenizer,\n",
        "        train_dataset = train_subset,\n",
        "        eval_dataset = eval_set,\n",
        "        max_seq_length = max_seq_length,\n",
        "        dataset_num_proc = 2,\n",
        "        packing = False,\n",
        "        data_collator = DataCollatorForLabelOnlyLoss(tokenizer=tokenizer, label_texts=CLASSES), # The custom data collator\n",
        "        compute_metrics = compute_eval_metrics, # The custom evaluation metrics\n",
        "        callbacks= config[\"callbacks\"], # Perform early stopping (for medium and hard)\n",
        "        args = TrainingArguments(\n",
        "            per_device_train_batch_size = 2,\n",
        "            gradient_accumulation_steps = 4,\n",
        "            warmup_ratio = config[\"warmup_ratio\"],\n",
        "            num_train_epochs = config[\"num_train_epochs\"],\n",
        "            learning_rate = config[\"learning_rate\"],\n",
        "            fp16 = not is_bfloat16_supported(),\n",
        "            bf16 = is_bfloat16_supported(),\n",
        "            logging_steps = config[\"eval_steps\"],\n",
        "            eval_steps = config[\"eval_steps\"],\n",
        "            eval_strategy='steps',\n",
        "            save_steps = config[\"eval_steps\"],\n",
        "            save_strategy='steps',\n",
        "            load_best_model_at_end=config[\"load_model\"],\n",
        "            metric_for_best_model='convergence_metric', # Early stopping based on the convergence metric\n",
        "            greater_is_better=True,\n",
        "            optim = \"adamw_8bit\",\n",
        "            weight_decay = 0.1,\n",
        "            lr_scheduler_type = \"cosine\",\n",
        "            seed = 546297,\n",
        "            output_dir = \"output-dir\",\n",
        "            run_name=f\"run_name\", # Specify wandb run name\n",
        "            report_to = \"none\", # Switch to \"wandb\" if desired\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer_stats = trainer.train()\n",
        "    print(trainer_stats)\n",
        "\n",
        "    # Compute the performance on the test set to get insight into model behaviour for every phase\n",
        "    y_pred_test = zero_shot_classification(test_set)\n",
        "    print(classification_report(y_test, y_pred_test, digits = 4))\n",
        "\n",
        "# Save the predictions of the final model\n",
        "pd.Series(y_pred_test).to_csv('llama2015_obs_ce_test.csv', index=False)"
      ],
      "metadata": {
        "id": "Cp7ydkDi1OmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the evaluation metrics and save the results for the train set\n",
        "train_class = split['train'].map(format_zero_shot_prompts, batched=True,)\n",
        "y_pred_train = zero_shot_classification(train_class)\n",
        "print(classification_report(y_train, y_pred_train, digits = 4))\n",
        "pd.Series(y_pred_train).to_csv('llama2015_obs_ce_train.csv', index=False)"
      ],
      "metadata": {
        "id": "LR46Sp7h1OmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the evaluation metrics and save the results for the validation set\n",
        "eval_class = split['test'].map(format_zero_shot_prompts, batched=True,)\n",
        "y_pred_val = zero_shot_classification(eval_class)\n",
        "print(classification_report(y_val, y_pred_val, digits = 4))\n",
        "pd.Series(y_pred_val).to_csv('llama2015_obs_ce_val.csv', index=False)"
      ],
      "metadata": {
        "id": "MIKt0kTC1OmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BBS"
      ],
      "metadata": {
        "id": "QkPbsh39QG1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create training buckets for BBS and use 'sl', 'swn', or 'ce' as a complexity measure in the last argument"
      ],
      "metadata": {
        "id": "xu-W3O9Xl2rP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_subsets = BBS(split['train'], 3, 'swn')"
      ],
      "metadata": {
        "id": "znx3Mwy0QH3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model with curriculum learning using BBS"
      ],
      "metadata": {
        "id": "zRVhfPQjmh7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the settings for every training phase, note that we train on the easy data for 1 epoch, while we train on the medium and hard data until convergence\n",
        "phase_configs = [\n",
        "    {\"name\": \"easy\", \"num_train_epochs\": 1,  \"learning_rate\": 2e-5,  \"warmup_ratio\": 0.1, \"eval_steps\": 20, \"callbacks\": [EarlyStoppingCallback(early_stopping_patience=100)], \"load_model\": False},\n",
        "    {\"name\": \"medium\", \"num_train_epochs\": 7, \"learning_rate\": 2e-5, \"warmup_ratio\": 0.06, \"eval_steps\": 25, \"callbacks\": [EarlyStoppingCallback(early_stopping_patience=5)], \"load_model\": True},\n",
        "    {\"name\": \"hard\", \"num_train_epochs\": 10, \"learning_rate\": 2e-5,\"warmup_ratio\": 0.03, \"eval_steps\": 30, \"callbacks\" : [EarlyStoppingCallback(early_stopping_patience=5)], \"load_model\": True}\n",
        "]\n",
        "\n",
        "# Iterate over the training phases\n",
        "for phase_idx, (subset, config) in enumerate(zip(train_subsets, phase_configs)):\n",
        "    print(f\"\\n=== Starting {config['name']} phase ===\")\n",
        "    train_subset = subset.map(format_fine_tune_prompts, batched=True, remove_columns=subset.column_names)\n",
        "\n",
        "    # Initialize the custom SFFTrainer for each phase\n",
        "    trainer = CustomSFTTrainer(\n",
        "        model = model,\n",
        "        tokenizer = tokenizer,\n",
        "        train_dataset = train_subset,\n",
        "        eval_dataset = eval_set,\n",
        "        max_seq_length = max_seq_length,\n",
        "        dataset_num_proc = 2,\n",
        "        packing = False,\n",
        "        data_collator = DataCollatorForLabelOnlyLoss(tokenizer=tokenizer, label_texts=CLASSES), # The custom data collator\n",
        "        compute_metrics = compute_eval_metrics, # The custom evaluation metrics\n",
        "        callbacks= config[\"callbacks\"], # Perform early stopping (for medium and hard)\n",
        "        args = TrainingArguments(\n",
        "            per_device_train_batch_size = 2,\n",
        "            gradient_accumulation_steps = 4,\n",
        "            warmup_ratio = config[\"warmup_ratio\"],\n",
        "            num_train_epochs = config[\"num_train_epochs\"],\n",
        "            learning_rate = config[\"learning_rate\"],\n",
        "            fp16 = not is_bfloat16_supported(),\n",
        "            bf16 = is_bfloat16_supported(),\n",
        "            logging_steps = config[\"eval_steps\"],\n",
        "            eval_steps = config[\"eval_steps\"],\n",
        "            eval_strategy='steps',\n",
        "            save_steps = config[\"eval_steps\"],\n",
        "            save_strategy='steps',\n",
        "            load_best_model_at_end=config[\"load_model\"],\n",
        "            metric_for_best_model='convergence_metric', # Early stopping based on the convergence metric\n",
        "            greater_is_better=True,\n",
        "            optim = \"adamw_8bit\",\n",
        "            weight_decay = 0.1,\n",
        "            lr_scheduler_type = \"cosine\",\n",
        "            seed = 546297,\n",
        "            output_dir = \"output-dir\",\n",
        "            run_name=f\"run_name\", # Specify wandb run name\n",
        "            report_to = \"none\", # Switch to \"wandb\" if desired\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer_stats = trainer.train()\n",
        "    print(trainer_stats)\n",
        "\n",
        "    # Compute the performance on the test set to get insight into model behaviour for every phase\n",
        "    y_pred_test = zero_shot_classification(test_set)\n",
        "    print(classification_report(y_test, y_pred_test, digits = 4))\n",
        "\n",
        "# Save the predictions of the final model\n",
        "pd.Series(y_pred_test).to_csv('llama2016_bs_ce_test.csv', index=False)"
      ],
      "metadata": {
        "id": "idtUwmw8Ft0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the evaluation metrics and save the results for the train set\n",
        "train_class = split['train'].map(format_zero_shot_prompts, batched=True,)\n",
        "y_pred_train = zero_shot_classification(train_class)\n",
        "print(classification_report(y_train, y_pred_train, digits = 4))\n",
        "pd.Series(y_pred_train).to_csv('llama2016_bs_ce_train.csv', index=False)"
      ],
      "metadata": {
        "id": "DxV3ezzPc4En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the evaluation metrics and save the results for the validation set\n",
        "eval_class = split['test'].map(format_zero_shot_prompts, batched=True,)\n",
        "y_pred_val = zero_shot_classification(eval_class)\n",
        "print(classification_report(y_val, y_pred_val, digits = 4))\n",
        "pd.Series(y_pred_val).to_csv('llama2016_bs_ce_val.csv', index=False)"
      ],
      "metadata": {
        "id": "RPyR9a0Ec7wC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}