{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook retrieves the k most similar training instances, for every test instance, by performing KATE.\n",
        "\n",
        "The notebook is inspired by:\n",
        "\n",
        "Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen, W. (2022). What makes good in-context\n",
        "examples for GPT-3? In 3rd Workshop on Knowledge Extraction and Integration for Deep Learning\n",
        "Architectures (DeeLIO 2022), pages 100â€“114. ACL.\n",
        "https://github.com/jiachangliu/KATEGPT3"
      ],
      "metadata": {
        "id": "BTvZM2h38lfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Packages"
      ],
      "metadata": {
        "id": "zZBu3OO62nfx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-1j8Y8q9YIs"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "import os\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functions"
      ],
      "metadata": {
        "id": "sZwoKexU2rEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Data"
      ],
      "metadata": {
        "id": "MAvvZrFs6NEW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2fyZSTj9uI4"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load and preprocess the XML file into a pandas DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load and parse XML file\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Extract data into a list of dictionaries\n",
        "    data = []\n",
        "\n",
        "    # Iterate through the XML structure and extract information form the reviews\n",
        "    for review in root.findall('Review'):\n",
        "        review_id = review.get('rid')\n",
        "\n",
        "        for sentence in review.findall('.//sentence'):\n",
        "            text = sentence.find('text').text\n",
        "\n",
        "            for opinion in sentence.findall('.//Opinion'):\n",
        "                aspect = opinion.get('target')\n",
        "                if aspect == 'NULL':\n",
        "                  aspect = ''\n",
        "\n",
        "                # Represent categories as aspect \"entity category\" for better to focus on the words during similarity computation.\n",
        "                category = opinion.get('category').lower().replace('#', ' ').replace('_',' ')\n",
        "                if category == 'food general':\n",
        "                    category = 'food style options'\n",
        "\n",
        "                data.append({\n",
        "                    \"sentence\": text,\n",
        "                    \"aspect\": aspect,\n",
        "                    \"category\": category,\n",
        "                    \"sentiment\": opinion.get('polarity')\n",
        "                })\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embed the data"
      ],
      "metadata": {
        "id": "rnu0D-Vt6O4s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fV2R_Y-w8V_k"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained SBERT model\n",
        "sbert_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "def get_sbert_embedding(text):\n",
        "    \"\"\"\n",
        "    Generate a sentence embedding using SBERT.\n",
        "    \"\"\"\n",
        "\n",
        "    return sbert_model.encode(text, convert_to_numpy=True, normalize_embeddings=True)\n",
        "\n",
        "def embed_data(df):\n",
        "    \"\"\"\n",
        "    Embed all elements in the DataFrame using SBERT.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a list to store embeddings\n",
        "    embedded_sentences = []\n",
        "    embedded_categories = []\n",
        "    embedded_terms = []\n",
        "\n",
        "    # Loop through each row in the DataFrame\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Embedding entries with SBERT\"):\n",
        "\n",
        "        sentence = row['sentence']\n",
        "        category = row['category']\n",
        "        aspect = row['aspect']\n",
        "\n",
        "        # Embed the sentences, aspect categories and aspect terms\n",
        "        sentence_embedding = get_sbert_embedding(sentence)\n",
        "        category_embedding = get_sbert_embedding(category)\n",
        "        aspect_embedding = get_sbert_embedding(aspect)\n",
        "\n",
        "        # Append the embeddings to the lists\n",
        "        embedded_sentences.append(sentence_embedding)\n",
        "        embedded_categories.append(category_embedding)\n",
        "        embedded_terms.append(aspect_embedding)\n",
        "\n",
        "    # Add the embeddings as a new column\n",
        "    df['sentence_embedding'] = embedded_sentences\n",
        "    df['category_embedding'] = embedded_categories\n",
        "    df['aspect_embedding'] = embedded_terms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KATE Algorithm"
      ],
      "metadata": {
        "id": "MA3M09LGDRcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def KATE(df_train, df_test, k):\n",
        "    \"\"\"\n",
        "    Retrieve the k most similar instance for each test instance using HKATE.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a list to store indices\n",
        "    list_of_indices = []\n",
        "    i = 0\n",
        "\n",
        "    # Load the embeddings of the training data\n",
        "    sentences_train = list(df_train['sentence_embedding'])\n",
        "    text_sentences_train = list(df_train['sentence'])\n",
        "\n",
        "    # Loop through each row in the DataFrame\n",
        "    for _, test_row in df_test.iterrows():\n",
        "\n",
        "      test_sentence = test_row['sentence_embedding']\n",
        "\n",
        "      # For every element of the test instance, compute the cosine similarities with the training data\n",
        "      sentence_similarities = cosine_similarity([test_sentence], sentences_train)[0]\n",
        "\n",
        "      # Order the training data based on similarity to the test instance (negative to sort on similarity in descending order)\n",
        "      similarities = [(text_sentences_train[i], sentence_similarities[i], i) for i in range(len(text_sentences_train))]\n",
        "      similarities.sort(key=lambda x: (-x[1], x[2])) #negative for descending order\n",
        "\n",
        "      unique_sentences = {}\n",
        "      top_k_results = []\n",
        "\n",
        "      # Add the most similar instance, with unique sentences, to the list of k most similar instances\n",
        "      for sen, simsen, idx in similarities:\n",
        "        if sen not in unique_sentences:\n",
        "          unique_sentences[sen] = (simsen, idx)\n",
        "          top_k_results.append(idx)\n",
        "          if len(top_k_results) == k:\n",
        "            break\n",
        "\n",
        "      list_of_indices.append(top_k_results)\n",
        "\n",
        "    # Add the list with k most similar instances as a new column\n",
        "    df_test['top_k_indices'] = list_of_indices"
      ],
      "metadata": {
        "id": "wIur55lbZ8V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaPXUcj2AMtQ"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Files"
      ],
      "metadata": {
        "id": "XqFX6FP3LqN8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run this code, first upload the files:\n",
        "\n",
        "*   '2015_Restaurants_Train.xml'\n",
        "*   '2015_Restaurants_Test.xml'\n",
        "*   '2016_Restaurants_Train.xml'\n",
        "*   '2016_Restaurants_Test.xml'"
      ],
      "metadata": {
        "id": "GotkRQN08P3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2015"
      ],
      "metadata": {
        "id": "FjYb4XMQ7CnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from XML file and embed the elements\n",
        "df_train_2015 = load_data('2015_Restaurants_Train.xml')\n",
        "embed_data(df_train_2015)\n",
        "df_test_2015 = load_data('2015_Restaurants_Test.xml')\n",
        "embed_data(df_test_2015)\n",
        "\n",
        "# Perform HKATE and save the list of top k=10 indices for later use\n",
        "KATE(df_train_2015,df_test_2015, 10)\n",
        "top_k_indices_kate_2015 = df_test_2015['top_k_indices']\n",
        "top_k_indices_kate_2015.to_csv('top_k_indices_2015.csv', index=False)"
      ],
      "metadata": {
        "id": "Eg019H8WvpzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2016"
      ],
      "metadata": {
        "id": "R17EvDvH7E-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from XML file and embed the elements\n",
        "df_train_2016 = load_data('2016_Restaurants_Train.xml')\n",
        "embed_data(df_train_2016)\n",
        "df_test_2016 = load_data('2016_Restaurants_Test.xml')\n",
        "embed_data(df_test_2016)\n",
        "\n",
        "# Perform KATE and save the list of top k=10 indices for later use\n",
        "KATE(df_train_2016,df_test_2016, 10)\n",
        "top_k_indices_kate_2016 = df_test_2016['top_k_indices']\n",
        "top_k_indices_kate_2016.to_csv('top_k_indices_2016.csv', index=False)"
      ],
      "metadata": {
        "id": "5JYeGNAGwJOk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}